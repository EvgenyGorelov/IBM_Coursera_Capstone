{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "\n# Advanced Data Science Capstone\n\n## Correlation of air pollution and Prevalence of Asthma bronchiale in Germany  \n\n## Feature Creation and Feature engineering", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### The deliverables\nThe deliverables of the current stage:\n\n - Spark DataFrames with disease prevalence column, county id, and some features extracted from air pollution data series for sensors located in corresponding county\n\n###  Feature creation\nThe basic features for air pollution levels are\n\n - Number of hours when pollutant concentration exceeded some certain value\n - Mean or Median concentration of the pollutant\n \n###  Feature quality check\n\n - Feature variance\n - Feature cross-correlation matrix\n \nThe necessary libraries and the data sets preprocessed at the ETL stage loaded:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190823064850-0000\nKERNEL_ID = 4dcec509-272c-4f5b-8d07-f31924dbc2a9\n"
                }
            ], 
            "source": "##### Libraries:\n#import pandas as pd\n#import numpy as np\n#import matplotlib.pyplot as plt\n#import re"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "###  Feature creation\n\nNow let's create some basic features, illustrating some integral quantities of air pollution over the year.\nFor the start the following features will be generated:\n - Average concentration of every kind of pollutant over the year (average over all sensors within the county)\n - 50th and 75th percentile of every kind pollutant over the year, that is also proportional to the number of days when pollutant concentration exceeded some certain value\n - The feature for the disease prevalence is constructed as presence of the county in the Nth (50th, 75th or 95th) percentile of the disease prevalence", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#sql view generation:\ndfAllLongSpark.createOrReplaceTempView(\"SensorsHour\")\ndfAsthmaSpark.createOrReplaceTempView(\"DiseaseCounty\")\n\n#spark.sql(\"select * from SensorsHour limit 10\").show()\n#spark.sql(\"select * from DiseaseCounty limit 10\").show() \n#dfAllLongSpark.printSchema()\n#dfAsthmaSpark.printSchema()"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n#dfPollutantPercentilesSpark = spark.sql(\"\"\"\n#SELECT \n# distinct \n#     Pollutant, CountyID,\n#     AVG(PollutantConc) over(PARTITION BY Pollutant, CountyID) AS Mean,\n#     percentile_approx(PollutantConc,  0.5) over(PARTITION BY Pollutant, CountyID) as Percentile50\n#--     ,\n#--     percentile_approx(PollutantConc, 0.75) over(PARTITION BY Pollutant, CountyID) as Percentile75\n#FROM SensorsHour\n#\"\"\") \n"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#dfPollutantPercentilesSpark.createOrReplaceTempView(\"PollutantPercentiles\")\n#spark.sql(\"select * from PollutantPercentiles limit 10\").show()"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#spark.sql(\"\"\"\n#       SELECT \n#         CountyID,\n#         (case \n#            WHEN DiseaseR >=\n#             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n#          THEN 1\n#          ELSE 0\n#         END \n#        ) as DiseaseRFeat\n#     FROM DiseaseCounty\n#\"\"\").show(290)"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#spark.sql(\"select * from PollutantPercentiles limit 10\").show()"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#spark.sql(\"\"\"\n#SELECT distinct \n#         CountyID,\n#         AVG(PollutantConc) over(PARTITION BY CountyID) AS NO\n#         FROM SensorsHour\n#         WHERE Pollutant='NO'\n#\"\"\").show(5)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Feature matrices to be created:\n#dfPolMeanLongDisease50perc = DiseaseFeaturePercentile(FeatureSetLongMean, 50.0)\n#dfPolMeanLongDisease75perc = DiseaseFeaturePercentile(FeatureSetLongMean, 75.0)\n#dfPolMeanLongDisease95perc = DiseaseFeaturePercentile(FeatureSetLongMean, 95.0)\n#\n#dfPolLongPerc75Disease50perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 50.0)\n#dfPolLongPerc75Disease75perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 75.0)\n#dfPolLongPerc75Disease95perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 95.0)\n\n# ListOfPollutantsLong = ['NO','NO2','PM1']"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease50perc : Pollutant mean; Disease level at or above 50th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolMeanLongDisease50perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.5) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease75perc : Pollutant mean; Disease level at or above 75th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolMeanLongDisease75perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease95perc : Pollutant mean; Disease level at or above 95th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolMeanLongDisease95perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.95) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolLongPerc75Disease50perc : Pollutant level at or above 75th percentile; Disease level at or above 50th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolLongPerc75Disease50perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.5) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolLongPerc75Disease75perc : Pollutant level at or above 75th percentile; Disease level at or above 75th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolLongPerc75Disease75perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolLongPerc75Disease95perc : Pollutant level at or above 75th percentile; Disease level at or above 95th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolLongPerc75Disease95perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.95) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------+------------+---+---+---+\n|CountyID|DiseaseRFeat| NO|NO2|PM1|\n+--------+------------+---+---+---+\n|       0|           0|  0|  0|  0|\n+--------+------------+---+---+---+\n\n+--------+------------+------------+-----------+------------------+\n|CountyID|DiseaseRFeat|          NO|        NO2|               PM1|\n+--------+------------+------------+-----------+------------------+\n|   14626|           0|       7.785|     20.616|23.673000000000002|\n|   14521|           0|     288.893|     92.927|              64.8|\n|   16051|           1| 27.11467361|38.37479019|       22.60592461|\n|   14730|           0|      19.833|     54.754|            130.99|\n|    1002|           0|      74.347|       58.3|           126.083|\n|    1051|           1|     188.201|     80.934|            95.755|\n|   15002|           0|    34.32875|   44.47335|          28.09545|\n|   16076|           0|250.02636719|74.93490601|      209.64552307|\n|   15001|           0|     281.628|    88.5767|          759.8985|\n|   15084|           0|    228.8535|    96.2292|          540.1065|\n+--------+------------+------------+-----------+------------------+\n\n"
                }
            ], 
            "source": "from pyspark.sql.functions import isnan, when, count, col\n\ndftmp = dfPolLongPerc75Disease75perc\n\ndftmp.select([count(when(isnan(c), c)).alias(c) for c in dftmp.columns]).show()\n\ndftmp.createOrReplaceTempView(\"PolLongPerc75Disease75perc\")\nspark.sql(\"select * from PolLongPerc75Disease75perc limit 10\").show()"
        }, 
        {
            "source": "### Writing Spark DataSrames as Parquet co COS", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dfPolMeanLongDisease50perc.write.parquet(cos.url('dfPolMeanLongDisease50perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolMeanLongDisease75perc.write.parquet(cos.url('dfPolMeanLongDisease75perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolMeanLongDisease95perc.write.parquet(cos.url('dfPolMeanLongDisease95perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\n\ndfPolLongPerc75Disease50perc.write.parquet(cos.url('dfPolLongPerc75Disease50perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolLongPerc75Disease75perc.write.parquet(cos.url('dfPolLongPerc75Disease75perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolLongPerc75Disease95perc.write.parquet(cos.url('dfPolLongPerc75Disease95perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}