{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "\n# Advanced Data Science Capstone\n\n## Correlation of air pollution and Prevalence of Asthma bronchiale in Germany  \n\n## Feature Creation and Feature engineering", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### The deliverables\nThe deliverables of the current stage:\n\n - Spark DataFrames with disease prevalence column, county id, and some features extracted from air pollution data series for sensors located in corresponding county\n\n###  Feature creation\nThe basic features for air pollution levels are\n\n - Number of hours when pollutant concentration exceeded some certain value\n - Mean or Median concentration of the pollutant\n \n###  Feature quality check\n\n - Feature variance\n - Feature cross-correlation matrix\n \nThe necessary libraries and the data sets preprocessed at the ETL stage loaded:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190822081237-0001\nKERNEL_ID = 79ad53a1-2597-4ba6-b2df-4485bc1acd78\n"
                }
            ], 
            "source": "##### Libraries:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "###  Feature creation\n\nNow let's create some basic features, illustrating some integral quantities of air pollution over the year.\nFor the start the following features will be generated:\n - Average concentration of every kind of pollutant over the year (average over all sensors within the county)\n - 50th and 75th percentile of every kind pollutant over the year, that is also proportional to the number of days when pollutant concentration exceeded some certain value", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------+---------+--------+\n|PollutantConc|Pollutant|CountyID|\n+-------------+---------+--------+\n|      15.5531|      NO2|   15002|\n|      15.5153|      NO2|   15002|\n|      19.5611|      NO2|   15002|\n|      20.2804|      NO2|   15002|\n|     28.63875|      NO2|   15002|\n|      50.6439|      NO2|   15002|\n|     62.99845|      NO2|   15002|\n|       55.207|      NO2|   15002|\n|      59.3478|      NO2|   15002|\n|      56.0626|      NO2|   15002|\n+-------------+---------+--------+\n\n+--------+--------+\n|CountyID|DiseaseR|\n+--------+--------+\n|   16056|     8.9|\n|   16072|     8.7|\n|    3451|     8.5|\n|   16073|     8.3|\n|    3151|     8.2|\n|    3403|     8.2|\n|    1059|     8.1|\n|    3156|     8.1|\n|    8128|     8.0|\n|    5570|     7.9|\n+--------+--------+\n\n"
                }
            ], 
            "source": "#dfAllSpark.createOrReplaceTempView(\"SensorsHour\")\n#SensorLocationSpark.createOrReplaceTempView(\"SensorsCounty\")\n\n#sql view generation:\ndfAllLongSpark.createOrReplaceTempView(\"SensorsHour\")\ndfAsthmaSpark.createOrReplaceTempView(\"DiseaseCounty\")\n\nspark.sql(\"select * from SensorsHour limit 10\").show()\n\nspark.sql(\"select * from DiseaseCounty limit 10\").show()          "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"drop TABLE PollutantPercentiles\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "     \n#--  (select percentile(PollutantConc,  0.5) over(PARTITION BY Pollutant, CountyID) from SensorsHour) as Percentile50\n#\n#--     group by Pollutant, CountyID)\n#--,\n#--      (select percentile(PollutantConc, 0.75) as Percentile75 from SensorsHour group by Pollutant, CountyID),\n\n\n#agent_id|payment_amount|\n#+--------+--------------+\n#|       a|          1000|\n#|       b|          1100|\n#\n# select agent_id, percentile_approx(payment_amount,0.95) as approxQuantile from df group by agent_id\n"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- PollutantConc: double (nullable = true)\n |-- Pollutant: string (nullable = true)\n |-- CountyID: long (nullable = true)\n\nroot\n |-- CountyID: long (nullable = true)\n |-- DiseaseR: double (nullable = true)\n\n"
                }
            ], 
            "source": "dfAllLongSpark.printSchema()\ndfAsthmaSpark.printSchema()"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- Pollutant: string (nullable = true)\n |-- CountyID: long (nullable = true)\n |-- Mean: double (nullable = true)\n |-- Percentile50: double (nullable = true)\n |-- Percentile75: double (nullable = true)\n\n"
                }
            ], 
            "source": "from pyspark.sql.types import *\n#StructType, StructField, IntegerType, StringType\n\nschema = StructType([\n    StructField(\"Pollutant\", StringType(), True), \n    StructField(\"CountyID\", LongType(), True),\n    StructField(\"Mean\", DoubleType(), True),\n    StructField(\"Percentile50\", DoubleType(), True),\n    StructField(\"Percentile75\", DoubleType(), True)    \n   ])\n\n# or df = sc.parallelize([]).toDF(schema)\n# Spark < 2.0 \n# sqlContext.createDataFrame([], schema)\n\ndfPollutantPercentilesSpark = spark.createDataFrame([], schema)\n\n\n#dfPollutantPercentilesSpark = sqlContext.createDataFrame(sc.emptyRDD())\ndfPollutantPercentilesSpark.createOrReplaceTempView(\"PollutantPercentiles\")\ndfPollutantPercentilesSpark.printSchema()"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---------+--------+----+------------+------------+\n|Pollutant|CountyID|Mean|Percentile50|Percentile75|\n+---------+--------+----+------------+------------+\n+---------+--------+----+------------+------------+\n\n"
                }
            ], 
            "source": "spark.sql(\"select * from PollutantPercentiles limit 10\").show()"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "KeyboardInterrupt", 
                    "evalue": "", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-14-b0039f8a3776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m      \u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPollutantConc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARTITION\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mPollutant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountyID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPercentile75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mSensorsHour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \"\"\").show(10)          \n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#\"\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/conda/miniconda36/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "#UPDATE table1 \n#SET a = t2.a, b = t2.b, .......\n#FROM table2 t2\n#WHERE table1.id = t2.id\n\n\n#spark.sql(\"INSERT INTO TABLE appendTest SELECT id, name, age, null, null FROM inputTable\");\n\n#spark.sql(\"\"\"INSERT INTO TABLE PollutantPercentiles \n\nspark.sql(\"\"\"\nSELECT distinct \n     Pollutant, CountyID,\n     AVG(PollutantConc) over(PARTITION BY Pollutant, CountyID) AS Mean,\n     percentile(PollutantConc,  0.5) over(PARTITION BY Pollutant, CountyID) as Percentile50,\n     percentile(PollutantConc, 0.75) over(PARTITION BY Pollutant, CountyID) as Percentile75\nFROM SensorsHour\n\"\"\").show(10)          \n\n#\"\"\") \n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"\"\"CREATE TABLE PollutantPercentiles AS(\nSELECT distinct \n     Pollutant, CountyID,\n     AVG(PollutantConc) over(PARTITION BY Pollutant, CountyID) AS Mean,\n     percentile(PollutantConc,  0.5) over(PARTITION BY Pollutant, CountyID) as Percentile50,\n     percentile(PollutantConc, 0.75) over(PARTITION BY Pollutant, CountyID) as Percentile75\nFROM SensorsHour)\n\"\"\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"select * from PollutantPercentiles limit 10\").show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"drop TABLE PolMeanLongDisease50perc\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease50perc : Pollutant mean; Disease level at or above 50th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\nspark.sql(\"\"\"\nCREATE TABLE PolMeanLongDisease50perc AS \n      (\n       SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n--           (select top 1 percentile_cont(.75) within group (order by DiseaseR) over() from DiseaseCounty) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty)\n\"\"\")\n              \n\n          \n# Feature matrices to be created:\n#dfPolMeanLongDisease50perc = DiseaseFeaturePercentile(FeatureSetLongMean, 50.0)\n#dfPolMeanLongDisease75perc = DiseaseFeaturePercentile(FeatureSetLongMean, 75.0)\n#dfPolMeanLongDisease95perc = DiseaseFeaturePercentile(FeatureSetLongMean, 95.0)\n#\n#dfPolLongPerc75Disease50perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 50.0)\n#dfPolLongPerc75Disease75perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 75.0)\n#dfPolLongPerc75Disease95perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 95.0)\n          \n# ListOfPollutantsDense = ['NO','NO2','NOx','O3','PM1']    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"select * from PolMeanLongDisease50perc limit 10\").show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"\"\"\nALTER TABLE PolMeanLongDisease50perc ADD COLUMNS (NO double, NO2 double, PM1 double)\n\"\"\").show()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.sql(\"\"\"\nUPDATE PolMeanLongDisease50perc\nSET PolMeanLongDisease50perc.NO=(SELECT PollutantPercentiles.Mean\n  FROM PollutantRercentiles\n  WHERE PollutantRercentiles.Pollutant='NO' AND PollutantRercentiles.CountyID=PolMeanLongDisease50perc.CountyID);\n\"\"\").show()"
        }, 
        {
            "source": "#### Mean pollutant concentration over the year\nThe quantity can be straightforward extracted using *.describe()* method of the data frame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dffAllSummary=dffAll.describe()\n\ndef MeanByPollutantCountyID(Pollutant, CountyID):\n    PollutantMasc='^.{8}' + Pollutant + '_'\n# collecting set of all sensors in the given county:    \n    CountyStationSet = SensorLocation.loc[SensorLocation['countyid']==CountyID]['SensorID'].tolist()\n    if (CountyStationSet == []) or ([col for col in dffAllSummary.columns if re.search('(^.{7})',col).group(1) in CountyStationSet if ('_'+Pollutant+'_') in col] == []):\n        return(np.nan)\n    try:\n        dffAllPollutantCountyID = dffAllSummary[[col for col in dffAllSummary.columns if re.search('(^.{7})',col).group(1) in CountyStationSet]].filter(regex=PollutantMasc,axis=1)\n        return(dffAllPollutantCountyID.loc[['mean']].mean(axis=1)[0])\n    except:    \n        return(np.nan)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ListOfPollutants = ['CO','NO','NO2','NOx','O3','PM1','PM2', 'PM10', 'SO2', 'CHB', 'CH4', 'C8H10', 'CHT', 'CO2']\n\nfor pollutant in ListOfPollutants:\n    ColIndex=pollutant+'mean'\n    dfDisease[ColIndex] =dfDisease.apply(lambda x: MeanByPollutantCountyID(pollutant, x['Regions-ID']), axis=1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# rewriting two functions above with SQL:\n# need: dfDiseasePolMean[pollutant] = df with columns: CountyID, pollutantMeanOverSensorsInThisCounty\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#dfDisease.head(53)\ndfDisease.isna().sum()"
        }, 
        {
            "source": "From the table above one can see, that only 5 pollutants (*NO, NO2, NOx, O3, PM1*) were measured in about a half of German counties (total number of counties mentioned in the *heart failures* and *asthma bronchiale* datasets is 402). Let's take a look, in how many counties all 6 pollutants were measured simultaneously:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "FeatureSetDenseMean = dfDisease[['Regions-ID','Wert','NOmean','NO2mean','NOxmean','O3mean','PM1mean']].dropna().reset_index(drop=True)\nFeatureSetDenseMean.columns=['CountyID','DiseaseR','NOmean','NO2mean','NOxmean','O3mean','PM1mean']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#FeatureSetDenseMean.tail(55)\nFeatureSetDenseMean.describe()\n#FeatureSetDenseMean.isna().sum()\n#FeatureSetDenseMean['NOmean'].mean()"
        }, 
        {
            "source": "From the table above one can conclude, that despite only about a third of German counties are included into the **FeatureSetDense** dataset, all the measured quantities have good variability. In order to illustrate it, the histograms are plotted:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "FeatureSetDenseMean.hist(column=['NOmean','NO2mean','NOxmean','O3mean','PM1mean']);"
        }, 
        {
            "source": "We can also maximize number of counties in the dataset, removing 2 of 5 pollutants. It is found, that for the current dataset it will lead to increase of observations up to 199, almost a half of the counties:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "FeatureSetLongMean = dfDisease[['Regions-ID','Wert','NOmean','NO2mean','PM1mean']].dropna().reset_index(drop=True)\nFeatureSetLongMean.columns=['CountyID','DiseaseR','NOmean','NO2mean','PM1mean']\nFeatureSetLongMean.describe()"
        }, 
        {
            "source": "Writing feature files:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!rm -rf Capstone.FeatureEng\n!mkdir Capstone.FeatureEng\nFeatureSetDenseMean.to_csv('Capstone.FeatureEng/Capstone.feature_eng.DenseMean.1.0.csv', index=False)\nFeatureSetLongMean.to_csv('Capstone.FeatureEng/Capstone.feature_eng.LongMean.1.0.csv', index=False)"
        }, 
        {
            "source": "#### Quantile-based features\nConstructing the global distribution of selected pollutant concentration over all counties of interest,\none can find e.g. 75th quantile of this pollutant concentration distribution, \nand than count number of hours, when this concentration was exceeded in each county.\nThis quantity is some generalization of the *\"number of days when the pollutant limit has been exceeded\"* quantity, because it tune itself to pollution levels available in the dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Function for Croppind of initial dffAll dataset to columns reflecting measurements \n# of pollutants in ListOfPollutants at counties in ListOfCountyIDs:\ndef CropPollutantsIDsTS(ListOfPollutants, ListOfCountyIDs):\n    CountyStationSet=[]\n    dffAllCrop=pd.DataFrame(dffAll['observation_period'])\n\n    for CountyID in ListOfCountyIDs:\n        CountyStationSet.extend(SensorLocation.loc[SensorLocation['countyid']==CountyID]['SensorID'].tolist())\n    for Pollutant in ListOfPollutants:\n        PollutantMasc='^.{8}' + Pollutant + '_' \n        dffAllCrop=dffAllCrop.join(dffAll[[col for col in dffAll.columns if re.search('(^.{7})',col).group(1) in CountyStationSet]].filter(regex=PollutantMasc,axis=1))\n    return(dffAllCrop)\n\n# Function for calculating Nth percentiles for pollutants in ListOfPollutants at counties in ListOfCountyIDs:\ndef ReturnNthPercentilePollutantsIDsTS(Percentile, ListOfPollutants, ListOfCountyIDs):\n    CountyStationSet=[]\n    NthPercentile=pd.DataFrame()\n\n    for CountyID in ListOfCountyIDs:\n        CountyStationSet.extend(SensorLocation.loc[SensorLocation['countyid']==CountyID]['SensorID'].tolist())\n    for Pollutant in ListOfPollutants:\n        PollutantMasc='^.{8}' + Pollutant + '_'\n        NthPercentile[Pollutant] = [(dffAll[[col for col in dffAll.columns if re.search('(^.{7})',col).group(1) in CountyStationSet]].filter(regex=PollutantMasc,axis=1)).stack().reset_index(drop=True).quantile(Percentile/100.0)]\n    return(NthPercentile)\n\n# Function for calculating Feature matrix, containing average (over sensors) number of hours for each county, when pollutant concentration\n# was exceeding Nth global percentiles for the pollutant at counties of interest (in ListOfCountyIDs):\ndef FeatureMatrixNthPercentile(Percentile, ListOfPollutants, ListOfCountyIDs):\n    NthPercentile=ReturnNthPercentilePollutantsIDsTS(Percentile, ListOfPollutants, ListOfCountyIDs)\n    dffAllCrop=CropPollutantsIDsTS(ListOfPollutants, ListOfCountyIDs)\n\n    FeatureMatrix=pd.DataFrame({'CountyID': dfDisease.loc[dfDisease['Regions-ID'].isin(ListOfCountyIDs)]['Regions-ID'], 'DiseaseR': dfDisease.loc[dfDisease['Regions-ID'].isin(ListOfCountyIDs)]['Wert']}).reset_index(drop=True)\n    for Pollutant in ListOfPollutants:\n        PollutantHoursExceed=[]\n        PollutantMasc='^.{8}' + Pollutant + '_'\n        for CountyID in ListOfCountyIDs:\n            CountyStationSet=[]\n            CountyStationSet.extend(SensorLocation.loc[SensorLocation['countyid']==CountyID]['SensorID'].tolist())\n\n            dffPollutantCounty = dffAllCrop[[col for col in dffAllCrop.columns if re.search('(^.{7})',col).group(1) in CountyStationSet]].filter(regex=PollutantMasc,axis=1)\n            NumPollutantStationsAtCounty = dffPollutantCounty.shape[1]\n            PollutantHoursExceed.append((dffPollutantCounty.stack().reset_index(drop=True)>NthPercentile[Pollutant][0]).reset_index(drop=True).sum()/NumPollutantStationsAtCounty)\n        FeatureMatrix = FeatureMatrix.reset_index(drop=True).join(pd.DataFrame({Pollutant : PollutantHoursExceed}))        \n    return(FeatureMatrix)"
        }, 
        {
            "source": "The same way as it was done for the pollutant mean as a feature, the quantile-based features with two types of pollutant lists (*Dense* and *Long*) and two percentile values (50 and 75) will be created:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ListOfPollutantsDense = ['NO','NO2','NOx','O3','PM1']\nListOfCountyIDsDense = FeatureSetDenseMean['CountyID']\nFeatureSetDensePerc50 = FeatureMatrixNthPercentile(50, ListOfPollutantsDense, ListOfCountyIDsDense)\nFeatureSetDensePerc75 = FeatureMatrixNthPercentile(75, ListOfPollutantsDense, ListOfCountyIDsDense)\n\nListOfPollutantsLong = ['NO','NO2','PM1']\nListOfCountyIDsLong = FeatureSetLongMean['CountyID']\nFeatureSetLongPerc50 = FeatureMatrixNthPercentile(50, ListOfPollutantsLong, ListOfCountyIDsLong)\nFeatureSetLongPerc75 = FeatureMatrixNthPercentile(75, ListOfPollutantsLong, ListOfCountyIDsLong)\n\n#FeatureSetDensePerc50.to_csv('Capstone.FeatureEng/Capstone.feature_eng.DensePerc50.1.0.csv', index=False)\n#FeatureSetDensePerc75.to_csv('Capstone.FeatureEng/Capstone.feature_eng.DensePerc75.1.0.csv', index=False)\n\n#FeatureSetLongPerc50.to_csv('Capstone.FeatureEng/Capstone.feature_eng.LongPerc50.1.0.csv', index=False)\n#FeatureSetLongPerc75.to_csv('Capstone.FeatureEng/Capstone.feature_eng.LongPerc75.1.0.csv', index=False)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#FeatureSetDensePerc50.tail(10)\nFeatureSetDensePerc50.head(10)\n#FeatureSetLongPerc50.tail(10)\n#FeatureSetDensePerc75.tail(10)\n#FeatureSetLongPerc75.tail(10)\n"
        }, 
        {
            "source": "###  Feature transformation\n\nIn this part the following notation is used:\n - *X, y* - features and labels of initial feature matrix.\n - *X_train0, X_test0, y_train0, y_test0* initial feature matrix, splitted into train and test sets\n - *X_train, X_test, y_train, y_test* normalized feature matrix, splitted into train and test sets\n - *X_train_XXX, X_test_XXX, y_train_XXX, y_test_XXX* transformed (XXX is the name of transformation) feature matrix, splitted into train and test sets\n \n\n#### Normalization\nFor use in the machine-learning and especially deep learning frameworks the features have to be normalized:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "FeatureSet = FeatureSetDensePerc50\n\n#from sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import decomposition\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX, y = FeatureSet.drop(['CountyID', 'DiseaseR'], axis=1), FeatureSet['DiseaseR']\nX_train0, X_test0, y_train0, y_test0 = train_test_split(X, y, random_state=0)\n\n\n# default scaler: maps to [0:1]\nscaler = MinMaxScaler()\n\nscaler.fit(X_train0)\n# apply transform\nX_train = scaler.transform(X_train0)\nX_test  = scaler.transform(X_test0)\n\n# inverse transform\ninverse = scaler.inverse_transform(X_train)\n# also, maybe lognorm-like distribution can be converted to ~normal with box-cox."
        }, 
        {
            "source": "checking inversibility of the scaling procedure:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(X_train0[0:4])\nprint(inverse[0:4])"
        }, 
        {
            "source": "#### Principal Component Analysis (PCA)\nThe 5 features from the initial dataset are projected onto 3 eigenvectors, corresponding to 3 extreme eigenvalues of the feature matrix:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pca = decomposition.PCA(n_components=3)\npca.fit(X_train0)\nprint(\"PCA explained variance:\", pca.explained_variance_ratio_)\nX_train_PCA = pca.transform(X_train0)\nX_test_PCA = pca.transform(X_test0)"
        }, 
        {
            "source": "#### Polynomial Features\nIn order to capture nonlinear behavior of the model,  one can extend the basis set from x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> to 1, x<sub>1</sub>, x<sub>1</sub><sup>2</sup>, x<sub>2</sub>, x<sub>2</sub><sup>2</sup>, x<sub>3</sub>, x<sub>3</sub><sup>2</sup>:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "FeatureSet = FeatureSetLongPerc50\nX, y = FeatureSet.drop(['CountyID', 'DiseaseR'], axis=1), FeatureSet['DiseaseR']\nX_train0, X_test0, y_train0, y_test0 = train_test_split(X, y, random_state=0)\n\n\npoly = PolynomialFeatures(2)\npoly.fit_transform(X_train0)\nX_train_UpToSquares = poly.transform(X_train0)\nX_test_UpToSquares = poly.transform(X_test0)"
        }, 
        {
            "source": "In order to treat potential interactions between the feature variables,  one can extend the basis set from x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> to 1, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, x<sup>1</sup>x<sub>2</sub>, x<sub>1</sub>x<sub>3</sub>, x<sub>2</sub>x<sub>3</sub>, x<sub>1</sub>x<sub>2</sub>x<sub>3</sub>:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "poly = PolynomialFeatures(degree=3, interaction_only=True)\npoly.fit_transform(X_train0)\nX_train_UpToCube = poly.transform(X_train0)\nX_test_UpToCube = poly.transform(X_test0)"
        }, 
        {
            "source": "#### Correlation matrix\nThe matrices with correlations between features can be plotted for both (Dense and Long) datasets:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "corr = FeatureSetDensePerc75.corr()\ncorr.style.background_gradient(cmap='coolwarm')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "corr = FeatureSetLongPerc75.corr()\ncorr.style.background_gradient(cmap='coolwarm')"
        }, 
        {
            "source": "It becomes clear, that the concentrations of different nitrogen oxides are highly correlated.\nSo, additional features engineering (e.g. polynomial) would be favorable w.r.t. keeping all the oxide's measurements.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Additional feature creation, for classification approaches\n\nAdditional feature for the disease prevalence is constructed as presence of the county in Nth percentile of the disease prevalence:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def DiseaseFeaturePercentile(FeatureSetDF, Percentile):\n    DiseasePercentile = (FeatureSetDF['DiseaseR']).quantile(Percentile/100.0)\n    dfFeatureOut = FeatureSetLongPerc75.copy(deep=True)\n    dfFeatureOut['DiseaseR'] = ((dfFeatureOut['DiseaseR'])<DiseasePercentile)\n    dfFeatureOut=dfFeatureOut.rename(columns = {'DiseaseR':'DiseaseRFeat'})\n    return(dfFeatureOut)\n\ndfPolMeanLongDisease50perc = DiseaseFeaturePercentile(FeatureSetLongMean, 50.0)\ndfPolMeanLongDisease75perc = DiseaseFeaturePercentile(FeatureSetLongMean, 75.0)\ndfPolMeanLongDisease95perc = DiseaseFeaturePercentile(FeatureSetLongMean, 95.0)\n\ndfPolLongPerc75Disease50perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 50.0)\ndfPolLongPerc75Disease75perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 75.0)\ndfPolLongPerc75Disease95perc = DiseaseFeaturePercentile(FeatureSetLongPerc75, 95.0)\n\ndfPolMeanLongDisease50perc.to_csv('Capstone.FeatureEng/Capstone.feature_eng.PolMeanLongDisease50perc.1.0.csv', index=False)\ndfPolMeanLongDisease75perc.to_csv('Capstone.FeatureEng/Capstone.feature_eng.PolMeanLongDisease75perc.1.0.csv', index=False)\ndfPolMeanLongDisease95perc.to_csv('Capstone.FeatureEng/Capstone.feature_eng.PolMeanLongDisease95perc.1.0.csv', index=False)\n\ndfPolLongPerc75Disease50perc.to_csv('Capstone.FeatureEng/Capstone.feature_eng.PolLongPerc75Disease50perc.1.0.csv', index=False)\ndfPolLongPerc75Disease75perc.to_csv('Capstone.FeatureEng/Capstone.feature_eng.PolLongPerc75Disease75perc.1.0.csv', index=False)\ndfPolLongPerc75Disease95perc.to_csv('Capstone.FeatureEng/Capstone.feature_eng.PolLongPerc75Disease95perc.1.0.csv', index=False)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}