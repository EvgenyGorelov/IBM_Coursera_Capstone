{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "\n# Advanced Data Science Capstone\n\n## Air pollution and prevalence of bronchial asthma in Germany  \n\n## ETL, Data cleansing", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### The deliverables\nThe deliverables of the current stage:\n\n - current notebook as the process documentation\n - Spark data frame of the \"wide\" type, containing time series of pollutants concentrations for every available sensor\n - Spark data frame of the \"long\" type, containing time series of pollutants concentrations, county id and a pollutant label\n - Spark data frame with disease prevalence column (bronchiale asthma) and a county id\n \n### ETL\n #### Data Sources\n  -  The officially published data sets by **Gesch\u00e4fts- und Koordinierungsstelle GovData**, the search engine is available at https://www.govdata.de/web/guest/suchen.\n  - Data stream **E1a** contains measured (Link to Data stream **D**) values of gas phase pollutants (e.g. Ozone, NO2, SO2, CO), particle pollutants (e.g. dust) and dust constituants (e.g. heavy metals, PAK in PM10, PM2.5, TSP) as well es total deposition (BULK), wet deposition and meteorologic data (e.g. temperature, wind, pressure)for every measurement location.\n  - The data for years 2013 - 2018 is currently available. For the project I will limit myself with 2016 data (due to limited availability of the health related data sets), however the method and the model are easily extendable for the data for other years.\n  - Compressed dataset is available at https://datahub.uba.de/server/rest/directories/arcgisforinspire/INSPIRE/aqd_MapServer/Daten/AQD_DE_E1a_2016.zip .\n #### Data cleansing\n  - The air quality data sets are claimed to be \"validated\", so most work for cleansing the data is already done.\n  - The incomplete files from the datasets (not having \"hour\" in the name) are ignored.\n  - Few missing values (below 10%) appearing in the time series as negative values of the pollutant concentrations will be imputed.\n  - The sensors with heavily corrupted data (above 10% of measurements) will be dropped.\n \n #### Enterprise data storage\n  - Saving Spark data frames to the Cloud Object Storage (COS) in the Parquet format.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190827152924-0001\nKERNEL_ID = b1fc56da-8f75-4007-a7f2-1952aea375c7\n"
                }
            ], 
            "source": "import urllib.request\nimport xml.etree.ElementTree as ET\nfrom lxml import etree\nimport pandas as pd\nimport numpy as np\n\nimport re, collections\nfrom io import StringIO\nimport os, fnmatch\n#, fastparquet\n\nimport matplotlib.pyplot as plt\n\ndef SelectAllXMLsensorID():\n    varFull = [s for s in AllTags if 'value' in s][0]\n    return([re.sub(r'[^a-zA-Z0-9:]*\\'{http(.*)$', r'', re.sub(r'^.*AQD\\/SPO.DE_', r'', str(varr.attrib))) for varr in Eroot.iter(varFull) if 'AQD' in str(varr.attrib)]) \n\n"
        }, 
        {
            "source": "Now the files with pollutant concentration time series for the given year will be loaded to the **dffAll** Pandas data frame of the **wide** format. During the load procedure **consistensy** of **files** and **column** names will be checked.\n\nFirst, all the necessary files are downloaded from the web:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Archive:  Capstone.rawData/AQD_DE_E1a_2016/AQD_DE_E1a_2016.zip\n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_NO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_NOx_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_NO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_O3_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_PM1_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_PM1_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_PM2_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_PM2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SH_2016_SO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_CO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_NO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_NO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_O3_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_PM1_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_PM1_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_PM2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SL_2016_SO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_CHB_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_CHT_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_NO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_NOx_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_NO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_O3_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_PM1_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_PM1_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_PM2_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_SN_2016_SO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_CHB_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_CHT_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_CO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_H2S_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_NO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_NOx_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_NO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_O3_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_PM1_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_PM1_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_PM2_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_PM2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_ST_2016_SO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_CHB_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_CHT_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_CO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_NO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_NOx_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_NO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_O3_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_PM1_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_PM2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_TH_2016_SO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_CO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_HG_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_NO2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_NOx_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_NO_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_O3_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_PM0_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_PM1_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_PM1_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_PM2_day.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_PM2_hour.xml  \n  inflating: Capstone.rawData/AQD_DE_E1a_2016/DE_UB_2016_SO2_hour.xml  \nArchive:  Capstone.rawData/AQD_DE_D_2016.zip\n  inflating: Capstone.rawData/DE_D_allInOne_metaMeasurements_2016.xml  \n  inflating: Capstone.rawData/DE_D_Model_2016.xml  \nArchive:  Capstone.rawData/GV100AD3107.zip\n  inflating: Capstone.rawData/GV100AD3107/Datensatzbeschreibung_GV100AD.pdf  \n  inflating: Capstone.rawData/GV100AD3107/GV100AD_310719.ASC  \n  inflating: Capstone.rawData/GV100AD3107/Hinweise.txt  \n"
                }
            ], 
            "source": "!rm -rf ./Capstone.rawData\n## Download and decompress the dataset itself:\n!mkdir Capstone.rawData\n#!ls -l Capstone.rawData/\n\n##### Pollution 2016\n!mkdir Capstone.rawData/AQD_DE_E1a_2016\nurllib.request.urlretrieve(\"https://datahub.uba.de/server/rest/directories/arcgisforinspire/INSPIRE/aqd_MapServer/Daten/AQD_DE_E1a_2016.zip\", \"Capstone.rawData/AQD_DE_E1a_2016.zip\")\n!mv Capstone.rawData/AQD_DE_E1a_2016.zip Capstone.rawData/AQD_DE_E1a_2016/\n!unzip Capstone.rawData/AQD_DE_E1a_2016/AQD_DE_E1a_2016.zip -d Capstone.rawData/\n!rm Capstone.rawData/AQD_DE_E1a_2016/AQD_DE_E1a_2016.zip\n\n##### Sensor locations 2016\nurllib.request.urlretrieve(\"https://datahub.uba.de/server/rest/directories/arcgisforinspire/INSPIRE/aqd_MapServer/Daten/AQD_DE_D_2016.zip\", \"Capstone.rawData/AQD_DE_D_2016.zip\")\n!unzip Capstone.rawData/AQD_DE_D_2016.zip -d Capstone.rawData/\n!rm Capstone.rawData/AQD_DE_D_2016.zip\n\n##### Prevalence of Asthma bronchiale 2016 \n!mkdir Capstone.rawData/Asthma_2016\nurllib.request.urlretrieve(\"https://www.versorgungsatlas.de/fileadmin/excel/data_id_92_kreis11_1_j_1451606400.xlsx\", \"Capstone.rawData/Asthma_2016/data_id_92_kreis11_1_j_1451606400.xlsx\")\n\n##### Town-county dataset:\nurllib.request.urlretrieve(\"https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/Archiv/GV100ADQ/GV100AD3107.zip?__blob=publicationFile\",\n                           \"Capstone.rawData/GV100AD3107.zip\")\n!mkdir Capstone.rawData/GV100AD3107\n!unzip Capstone.rawData/GV100AD3107.zip -d Capstone.rawData/GV100AD3107/\n!rm Capstone.rawData/GV100AD3107.zip"
        }, 
        {
            "source": "Then the pollutant concentration *xml* files are parsed and hourly averaged values of pollutants concentrations are stored in the **dffAll** Pandas DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of files in the dataset 51\n"
                }
            ], 
            "source": "AirE1aDir='Capstone.rawData/AQD_DE_E1a_2016/'\n\n#!ls Capstone.rawData/AQD_DE_E1a_2016/*hour*\nFilesHour=[]\n\nfor file in os.listdir(AirE1aDir):\n    if fnmatch.fnmatch(file, '*hour*'):\n        FilesHour.append(file)\nprint(\"Number of files in the dataset\", len(FilesHour))\n\n# shortening the process for debugging purposes\n#FilesHour=FilesHour[0:3]        \n\nNumHoursInYear=8760 # 8760 hours in the year\nNumHoursInYear=8784 # 8784 hours in the leap year 2016\n\ndffAll=pd.DataFrame(index=range(0,NumHoursInYear))  \n\n# add First column with Observation Times:\ndff=[]  # Temporary list for DataFrames\n\nfile=FilesHour[0]\nEtree = ET.parse(AirE1aDir+file)\nEroot = Etree.getroot()\nEroot.tag\nEroot.attrib\nAllTags = [elem.tag for elem in Eroot.iter()]\nvarFull = [s for s in AllTags if 'values' in s][0]\nfor varr in Eroot.iter(varFull):\n    dff.append(pd.read_csv(StringIO((varr.text).replace(\"@@\",\"\\n\")), sep=\",\", header=None))\ndffAll=pd.concat([dffAll, dff[0][[0]]], axis=1)\ndffAll.columns=['observation_period']\n\n\n# get all tags in xml file; Note, that the actual data is kept as a TEXT of *values* tags \nfor file in FilesHour:\n    Etree = ET.parse(AirE1aDir+file)\n    Eroot = Etree.getroot()\n    Eroot.tag\n    Eroot.attrib\n    AllTags = [elem.tag for elem in Eroot.iter()]\n    \n    ColNamesExp=SelectAllXMLsensorID()\n# Compare column names with file names, they should encode same country, state and pollutant\n    for ColName in ColNamesExp:\n        if ((ColName[0:2]!=file[0:2]) or (ColName[2:4]!=file[3:5]) or (ColName[8:11]!=file[11:14])):\n            print(\"Inconsistency in file and column names: \", file, ColName)\n            exit()\n    \n    varFull = [s for s in AllTags if 'values' in s][0]\n    \n    dff=[] # Temporary list for DataFrames\n# reading actual pollutant data fiom the text field:    \n    for varr in Eroot.iter(varFull):\n        dff.append(pd.read_csv(StringIO((varr.text).replace(\"@@\",\"\\n\")), sep=\",\", header=None))\n\n# checking, that measurment timestamps are identical in the files read       \n    for s in range(0,len(dff)):\n        if not (dffAll['observation_period']).equals(dff[s][0]):\n            print(\"Inconsistency of observation times in the following files: \", file, FilesHour[0])\n            exit()\n\n        \n# select column 4 - pollutant concentration:\n    dff=pd.concat([dff[s][4] for s in range(0,len(dff))], axis=1)\n    dff.columns=ColNamesExp\n   \n    dffAll=pd.concat([dffAll, dff], axis=1)"
        }, 
        {
            "source": "Now check the data set size and print a summary:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Memory usage:  35.25080871582031  MB\n"
                }, 
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DESH008_NO2_dataGroup1</th>\n      <th>DESH022_NO2_dataGroup1</th>\n      <th>DESH023_NO2_dataGroup1</th>\n      <th>DESH025_NO2_dataGroup1</th>\n      <th>DESH027_NO2_dataGroup1</th>\n      <th>DESH028_NO2_dataGroup1</th>\n      <th>DESH030_NO2_dataGroup1</th>\n      <th>DESH033_NO2_dataGroup1</th>\n      <th>DESH035_NO2_dataGroup1</th>\n      <th>DESH052_NO2_dataGroup1</th>\n      <th>...</th>\n      <th>DEUB029_PM1_dataGroup1</th>\n      <th>DEUB030_PM1_dataGroup1</th>\n      <th>DEUB005_PM2_dataGroup1</th>\n      <th>DEUB001_SO2_dataGroup1</th>\n      <th>DEUB004_SO2_dataGroup1</th>\n      <th>DEUB005_SO2_dataGroup1</th>\n      <th>DEUB028_SO2_dataGroup1</th>\n      <th>DEUB029_SO2_dataGroup1</th>\n      <th>DEUB030_SO2_dataGroup1</th>\n      <th>DEUB046_SO2_dataGroup1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>...</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n      <td>8784.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.517775</td>\n      <td>32.406144</td>\n      <td>12.276538</td>\n      <td>34.838776</td>\n      <td>38.864838</td>\n      <td>32.474424</td>\n      <td>39.616351</td>\n      <td>15.206867</td>\n      <td>21.245121</td>\n      <td>63.749169</td>\n      <td>...</td>\n      <td>-87.290716</td>\n      <td>-25.283687</td>\n      <td>-28.934937</td>\n      <td>-55.997823</td>\n      <td>-64.135568</td>\n      <td>-49.471064</td>\n      <td>-118.446669</td>\n      <td>-51.785409</td>\n      <td>-57.245443</td>\n      <td>-41.084183</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>50.050559</td>\n      <td>44.066237</td>\n      <td>48.330659</td>\n      <td>51.156795</td>\n      <td>65.114026</td>\n      <td>30.816056</td>\n      <td>71.091519</td>\n      <td>50.981938</td>\n      <td>51.424199</td>\n      <td>55.293691</td>\n      <td>...</td>\n      <td>298.946758</td>\n      <td>192.009980</td>\n      <td>191.831127</td>\n      <td>230.456606</td>\n      <td>245.820160</td>\n      <td>218.321263</td>\n      <td>323.767133</td>\n      <td>222.940043</td>\n      <td>234.069962</td>\n      <td>201.453804</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>...</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n      <td>-999.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.720500</td>\n      <td>19.265000</td>\n      <td>6.868500</td>\n      <td>23.445750</td>\n      <td>23.618000</td>\n      <td>18.396500</td>\n      <td>24.093000</td>\n      <td>9.337750</td>\n      <td>14.421750</td>\n      <td>33.478750</td>\n      <td>...</td>\n      <td>2.330000</td>\n      <td>5.780000</td>\n      <td>3.960000</td>\n      <td>0.190000</td>\n      <td>0.340000</td>\n      <td>0.280000</td>\n      <td>0.250000</td>\n      <td>0.260000</td>\n      <td>0.500000</td>\n      <td>0.927500</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.314500</td>\n      <td>30.672500</td>\n      <td>11.364500</td>\n      <td>36.152500</td>\n      <td>38.806000</td>\n      <td>30.105500</td>\n      <td>39.991500</td>\n      <td>14.422000</td>\n      <td>21.076000</td>\n      <td>57.629500</td>\n      <td>...</td>\n      <td>7.290000</td>\n      <td>9.875000</td>\n      <td>6.220000</td>\n      <td>0.310000</td>\n      <td>0.420000</td>\n      <td>0.390000</td>\n      <td>0.370000</td>\n      <td>0.440000</td>\n      <td>0.630000</td>\n      <td>1.170000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.121250</td>\n      <td>46.190250</td>\n      <td>18.579500</td>\n      <td>48.975250</td>\n      <td>57.580000</td>\n      <td>44.631250</td>\n      <td>58.782000</td>\n      <td>22.207000</td>\n      <td>30.282500</td>\n      <td>91.861750</td>\n      <td>...</td>\n      <td>14.220000</td>\n      <td>15.482500</td>\n      <td>10.700000</td>\n      <td>0.360000</td>\n      <td>0.540000</td>\n      <td>0.670000</td>\n      <td>0.540000</td>\n      <td>0.690000</td>\n      <td>0.850000</td>\n      <td>1.460000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>68.899000</td>\n      <td>126.621000</td>\n      <td>83.155000</td>\n      <td>113.510000</td>\n      <td>152.249000</td>\n      <td>137.767000</td>\n      <td>160.504000</td>\n      <td>83.625000</td>\n      <td>80.934000</td>\n      <td>228.703000</td>\n      <td>...</td>\n      <td>79.210000</td>\n      <td>88.690000</td>\n      <td>95.220000</td>\n      <td>4.390000</td>\n      <td>3.730000</td>\n      <td>14.760000</td>\n      <td>12.020000</td>\n      <td>32.680000</td>\n      <td>13.650000</td>\n      <td>10.740000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows \u00d7 525 columns</p>\n</div>", 
                        "text/plain": "       DESH008_NO2_dataGroup1  DESH022_NO2_dataGroup1  DESH023_NO2_dataGroup1  \\\ncount             8784.000000             8784.000000             8784.000000   \nmean                 7.517775               32.406144               12.276538   \nstd                 50.050559               44.066237               48.330659   \nmin               -999.000000             -999.000000             -999.000000   \n25%                  4.720500               19.265000                6.868500   \n50%                  7.314500               30.672500               11.364500   \n75%                 12.121250               46.190250               18.579500   \nmax                 68.899000              126.621000               83.155000   \n\n       DESH025_NO2_dataGroup1  DESH027_NO2_dataGroup1  DESH028_NO2_dataGroup1  \\\ncount             8784.000000             8784.000000             8784.000000   \nmean                34.838776               38.864838               32.474424   \nstd                 51.156795               65.114026               30.816056   \nmin               -999.000000             -999.000000             -999.000000   \n25%                 23.445750               23.618000               18.396500   \n50%                 36.152500               38.806000               30.105500   \n75%                 48.975250               57.580000               44.631250   \nmax                113.510000              152.249000              137.767000   \n\n       DESH030_NO2_dataGroup1  DESH033_NO2_dataGroup1  DESH035_NO2_dataGroup1  \\\ncount             8784.000000             8784.000000             8784.000000   \nmean                39.616351               15.206867               21.245121   \nstd                 71.091519               50.981938               51.424199   \nmin               -999.000000             -999.000000             -999.000000   \n25%                 24.093000                9.337750               14.421750   \n50%                 39.991500               14.422000               21.076000   \n75%                 58.782000               22.207000               30.282500   \nmax                160.504000               83.625000               80.934000   \n\n       DESH052_NO2_dataGroup1  ...  DEUB029_PM1_dataGroup1  \\\ncount             8784.000000  ...             8784.000000   \nmean                63.749169  ...              -87.290716   \nstd                 55.293691  ...              298.946758   \nmin               -999.000000  ...             -999.000000   \n25%                 33.478750  ...                2.330000   \n50%                 57.629500  ...                7.290000   \n75%                 91.861750  ...               14.220000   \nmax                228.703000  ...               79.210000   \n\n       DEUB030_PM1_dataGroup1  DEUB005_PM2_dataGroup1  DEUB001_SO2_dataGroup1  \\\ncount             8784.000000             8784.000000             8784.000000   \nmean               -25.283687              -28.934937              -55.997823   \nstd                192.009980              191.831127              230.456606   \nmin               -999.000000             -999.000000             -999.000000   \n25%                  5.780000                3.960000                0.190000   \n50%                  9.875000                6.220000                0.310000   \n75%                 15.482500               10.700000                0.360000   \nmax                 88.690000               95.220000                4.390000   \n\n       DEUB004_SO2_dataGroup1  DEUB005_SO2_dataGroup1  DEUB028_SO2_dataGroup1  \\\ncount             8784.000000             8784.000000             8784.000000   \nmean               -64.135568              -49.471064             -118.446669   \nstd                245.820160              218.321263              323.767133   \nmin               -999.000000             -999.000000             -999.000000   \n25%                  0.340000                0.280000                0.250000   \n50%                  0.420000                0.390000                0.370000   \n75%                  0.540000                0.670000                0.540000   \nmax                  3.730000               14.760000               12.020000   \n\n       DEUB029_SO2_dataGroup1  DEUB030_SO2_dataGroup1  DEUB046_SO2_dataGroup1  \ncount             8784.000000             8784.000000             8784.000000  \nmean               -51.785409              -57.245443              -41.084183  \nstd                222.940043              234.069962              201.453804  \nmin               -999.000000             -999.000000             -999.000000  \n25%                  0.260000                0.500000                0.927500  \n50%                  0.440000                0.630000                1.170000  \n75%                  0.690000                0.850000                1.460000  \nmax                 32.680000               13.650000               10.740000  \n\n[8 rows x 525 columns]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "print(\"Memory usage: \", (dffAll.memory_usage(index=True).sum()/1048576.0), \" MB\")\ndffAll.describe()"
        }, 
        {
            "source": "Now we have **wide** data frame, containing timeseries of all pollutant concentrations for all sensors. The pollutant type and the sensor ID are encoded in column names. The minimal value of pollutant concentrations *-999.0* is equivalent to *NA* and will be imputted, as well as all negative values (the concentration can not be negative). The limit for imputation will be set to 876, i.e. *NA* sequences exceeding 10% of the year will not be imputted. Since the number of heavily corrupted columns is below 2%, they will be dropped in favor to the information quality:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/ipykernel/__main__.py:3: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n  app.launch_new_instance()\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The number of corrupted columns is  10  of  526\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/ibm/conda/miniconda36/lib/python3.6/site-packages/pandas/core/arrays/datetimes.py:1172: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n  \"will drop timezone information.\", UserWarning)\n"
                }, 
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>observation_period</th>\n      <th>DESH008_NO2_dataGroup1</th>\n      <th>DESH022_NO2_dataGroup1</th>\n      <th>DESH023_NO2_dataGroup1</th>\n      <th>DESH025_NO2_dataGroup1</th>\n      <th>DESH027_NO2_dataGroup1</th>\n      <th>DESH028_NO2_dataGroup1</th>\n      <th>DESH030_NO2_dataGroup1</th>\n      <th>DESH033_NO2_dataGroup1</th>\n      <th>DESH035_NO2_dataGroup1</th>\n      <th>...</th>\n      <th>DEUB029_PM1_dataGroup1</th>\n      <th>DEUB030_PM1_dataGroup1</th>\n      <th>DEUB005_PM2_dataGroup1</th>\n      <th>DEUB001_SO2_dataGroup1</th>\n      <th>DEUB004_SO2_dataGroup1</th>\n      <th>DEUB005_SO2_dataGroup1</th>\n      <th>DEUB028_SO2_dataGroup1</th>\n      <th>DEUB029_SO2_dataGroup1</th>\n      <th>DEUB030_SO2_dataGroup1</th>\n      <th>DEUB046_SO2_dataGroup1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8781</th>\n      <td>2016-12-31 21:00</td>\n      <td>24.274</td>\n      <td>22.780</td>\n      <td>33.092</td>\n      <td>28.640</td>\n      <td>29.738</td>\n      <td>28.722</td>\n      <td>35.438</td>\n      <td>25.233</td>\n      <td>26.793</td>\n      <td>...</td>\n      <td>3.50</td>\n      <td>8.87</td>\n      <td>11.67</td>\n      <td>0.21</td>\n      <td>0.46</td>\n      <td>1.16</td>\n      <td>0.33</td>\n      <td>0.35</td>\n      <td>0.47</td>\n      <td>1.07</td>\n    </tr>\n    <tr>\n      <th>8782</th>\n      <td>2016-12-31 22:00</td>\n      <td>22.592</td>\n      <td>21.262</td>\n      <td>33.049</td>\n      <td>27.886</td>\n      <td>29.738</td>\n      <td>29.114</td>\n      <td>37.843</td>\n      <td>23.267</td>\n      <td>28.229</td>\n      <td>...</td>\n      <td>7.58</td>\n      <td>13.40</td>\n      <td>12.11</td>\n      <td>0.20</td>\n      <td>0.47</td>\n      <td>1.52</td>\n      <td>0.29</td>\n      <td>0.35</td>\n      <td>0.62</td>\n      <td>1.21</td>\n    </tr>\n    <tr>\n      <th>8783</th>\n      <td>2016-12-31 23:00</td>\n      <td>21.737</td>\n      <td>20.361</td>\n      <td>32.630</td>\n      <td>28.721</td>\n      <td>29.738</td>\n      <td>30.624</td>\n      <td>36.119</td>\n      <td>20.998</td>\n      <td>31.436</td>\n      <td>...</td>\n      <td>18.37</td>\n      <td>12.87</td>\n      <td>11.88</td>\n      <td>0.19</td>\n      <td>0.44</td>\n      <td>1.69</td>\n      <td>0.32</td>\n      <td>0.39</td>\n      <td>0.80</td>\n      <td>1.25</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows \u00d7 516 columns</p>\n</div>", 
                        "text/plain": "     observation_period  DESH008_NO2_dataGroup1  DESH022_NO2_dataGroup1  \\\n8781   2016-12-31 21:00                  24.274                  22.780   \n8782   2016-12-31 22:00                  22.592                  21.262   \n8783   2016-12-31 23:00                  21.737                  20.361   \n\n      DESH023_NO2_dataGroup1  DESH025_NO2_dataGroup1  DESH027_NO2_dataGroup1  \\\n8781                  33.092                  28.640                  29.738   \n8782                  33.049                  27.886                  29.738   \n8783                  32.630                  28.721                  29.738   \n\n      DESH028_NO2_dataGroup1  DESH030_NO2_dataGroup1  DESH033_NO2_dataGroup1  \\\n8781                  28.722                  35.438                  25.233   \n8782                  29.114                  37.843                  23.267   \n8783                  30.624                  36.119                  20.998   \n\n      DESH035_NO2_dataGroup1  ...  DEUB029_PM1_dataGroup1  \\\n8781                  26.793  ...                    3.50   \n8782                  28.229  ...                    7.58   \n8783                  31.436  ...                   18.37   \n\n      DEUB030_PM1_dataGroup1  DEUB005_PM2_dataGroup1  DEUB001_SO2_dataGroup1  \\\n8781                    8.87                   11.67                    0.21   \n8782                   13.40                   12.11                    0.20   \n8783                   12.87                   11.88                    0.19   \n\n      DEUB004_SO2_dataGroup1  DEUB005_SO2_dataGroup1  DEUB028_SO2_dataGroup1  \\\n8781                    0.46                    1.16                    0.33   \n8782                    0.47                    1.52                    0.29   \n8783                    0.44                    1.69                    0.32   \n\n      DEUB029_SO2_dataGroup1  DEUB030_SO2_dataGroup1  DEUB046_SO2_dataGroup1  \n8781                    0.35                    0.47                    1.07  \n8782                    0.35                    0.62                    1.21  \n8783                    0.39                    0.80                    1.25  \n\n[3 rows x 516 columns]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dffAll[dffAll.loc[:, dffAll.columns != 'observation_period'] < 0.0] = np.NaN\ndffAll.interpolate(method='linear', inplace=True, axis=0, limit=876, limit_direction='both')\nprint('The number of corrupted columns is ', len(dffAll.isna().sum().nonzero()[0]), ' of ', len(dffAll.columns))\ndffAll = dffAll.dropna(axis=1)\ndffAll['observation_period']=pd.to_datetime(dffAll['observation_period'])\ndffAll['observation_period']=dffAll['observation_period'].dt.to_period('H')\n#dffAll['observation_period'][0].end_time\ndffAll.tail(3)"
        }, 
        {
            "source": "### Saving Air Pollution DataFrame to the COS\nNow we can save the resulting dataset to the Cloud Object Storage for the further use:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import SparkSession\n\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\nspark = SparkSession.builder.getOrCreate()\n\ndfAllSpark = spark.createDataFrame(dffAll.drop('observation_period', axis = 1))\ndfAllSpark.write.parquet(cos.url('dffAll.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\n#dfAllSpark = spark.read.parquet(cos.url('dffAll.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\n#dfAllSpark.printSchema()\n#dfAllSpark.show()"
        }, 
        {
            "source": "### Sensor Locations\nIn order to use the spatial data one should have a coordinates of the air pollution sensors.\nFor the current study the county index for every individual sensor is needed. \n\nAll the sensors IDs and the town names of the sensors locations are read into the **SensorLocation** DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SensorID</th>\n      <th>SensorTown</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>762</th>\n      <td>DEUB005</td>\n      <td>L\u00fcder</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>DEUB028</td>\n      <td>Zingst</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>DEUB029</td>\n      <td>Suhl</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>DEUB030</td>\n      <td>Stechlin</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>DEUB044</td>\n      <td>Garmisch-Partenkirchen</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "    SensorID              SensorTown\n762  DEUB005                   L\u00fcder\n763  DEUB028                  Zingst\n764  DEUB029                    Suhl\n765  DEUB030                Stechlin\n766  DEUB044  Garmisch-Partenkirchen"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# pick all tags from the XML file\nEtree = etree.parse(\"Capstone.rawData/DE_D_allInOne_metaMeasurements_2016.xml\")\nEroot = Etree.getroot()\nEroot.tag\nEroot.attrib\nAllTags = [elem.tag for elem in Eroot.iter()]\n\n# get correct tag names for 'municipality', 'EUStationCode' and 'featureMember':\nvarMUN = [s for s in AllTags if 'municipality' in s][0]\nvarID  = [s for s in AllTags if 'EUStationCode' in s][0]\nvarFeatMem = [s for s in AllTags if 'featureMember' in s][0]\n\nIDs=[]\nMUNs=[]\n# read 'municipality' and 'EUStationCode' to SensorLocation dataframe:\nfor varr in Eroot.iter(varFeatMem):\n    for child in varr.iter(varMUN):\n        MUNs.append(child.text)\n        for child2 in varr.iter(varID):\n            IDs.append(child2.text)\nSensorLocation=pd.DataFrame({'SensorID': IDs, 'SensorTown': MUNs})\nSensorLocation.tail(5)"
        }, 
        {
            "source": "In order to map the town names to the county names used in the health indicators data sets, the town-county DataFrame **dfCT** is created. It contains 5-digit county-id (not unique, but characterizing counties in some vicinity), name of town and county: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CountyID</th>\n      <th>town</th>\n      <th>county</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16116</th>\n      <td>16077</td>\n      <td>Starkenberg</td>\n      <td>Schm\u00f6lln/Th\u00fcr.</td>\n    </tr>\n    <tr>\n      <th>16117</th>\n      <td>16077</td>\n      <td>Thonhausen</td>\n      <td>Schm\u00f6lln/Th\u00fcr.</td>\n    </tr>\n    <tr>\n      <th>16118</th>\n      <td>16077</td>\n      <td>Treben</td>\n      <td>Schm\u00f6lln/Th\u00fcr.</td>\n    </tr>\n    <tr>\n      <th>16119</th>\n      <td>16077</td>\n      <td>Vollmershain</td>\n      <td>Schm\u00f6lln/Th\u00fcr.</td>\n    </tr>\n    <tr>\n      <th>16120</th>\n      <td>16077</td>\n      <td>Windischleuba</td>\n      <td>Schm\u00f6lln/Th\u00fcr.</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "       CountyID           town          county\n16116     16077    Starkenberg  Schm\u00f6lln/Th\u00fcr.\n16117     16077     Thonhausen  Schm\u00f6lln/Th\u00fcr.\n16118     16077         Treben  Schm\u00f6lln/Th\u00fcr.\n16119     16077   Vollmershain  Schm\u00f6lln/Th\u00fcr.\n16120     16077  Windischleuba  Schm\u00f6lln/Th\u00fcr."
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "columns = [(10, 15), (22, 71), (72, 121)]\ndfCT = pd.read_fwf(\"Capstone.rawData/GV100AD3107/GV100AD_310719.ASC\", \n                     colspecs=columns, names=['CountyID','town','county'],\n                     encoding=\"iso8859_1\")\ndfCT=dfCT.fillna(method='ffill')\n\ndfCT['town'] = dfCT['town'].str.split(\",\").str[0]\ndfCT.tail(5)"
        }, 
        {
            "source": "### Prevalence of bronchial asthma\nThe central data frame of the model will contain list of counties, prevalence of disease(s) in this counties, and the set of air-pollution-based features. \nFirst the *Prevalence of bronchial asthma* dataset is loaded: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "xls sheet names:  ['Hintergrundinformationen', 'Daten']\n      Region  Regions-ID  KV             Kreistyp  Wert  Bundeswert\n0   Eisenach       16056  TH    L\u00e4ndliches Umland   8.9         5.7\n1  Sonneberg       16072  TH      L\u00e4ndlicher Raum   8.7         5.7\n2  Ammerland        3451  NI  Verdichtetes Umland   8.5         5.7\nNumber of duplicates in Regions-ID column:  0\n"
                }
            ], 
            "source": "xlsx_file = pd.ExcelFile(\"Capstone.rawData/Asthma_2016/data_id_92_kreis11_1_j_1451606400.xlsx\")\nprint(\"xls sheet names: \",xlsx_file.sheet_names)\ndfAsthma = xlsx_file.parse('Daten', header=3, decimal=\",\") \nprint(dfAsthma.head(3))\nprint(\"Number of duplicates in Regions-ID column: \", dfAsthma.duplicated(['Regions-ID']).sum())"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CountyID</th>\n      <th>DiseaseR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16056</td>\n      <td>8.9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16072</td>\n      <td>8.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3451</td>\n      <td>8.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16073</td>\n      <td>8.3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3151</td>\n      <td>8.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   CountyID  DiseaseR\n0     16056       8.9\n1     16072       8.7\n2      3451       8.5\n3     16073       8.3\n4      3151       8.2"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dfAsthma = dfAsthma.drop(['Region', 'KV', 'Kreistyp', 'Bundeswert'], axis=1)\ndfAsthma.columns=['CountyID','DiseaseR']\ndfAsthma.head(5)"
        }, 
        {
            "source": "The mapping of sensor positions to counties is done by setting the **CountyID** to every **sensorID** in the **SensorLocation** dataframe:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "SensorLocation = (SensorLocation.join(dfCT[['CountyID','town']].set_index('town'),\n                                      on='SensorTown')).drop_duplicates(subset=['SensorID'])"
        }, 
        {
            "source": "Checking the resulting table one can see that 23 of 767 entries have not resolved **CountyID**:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Total number of sensors:  SensorID      767\nSensorTown    767\nCountyID      744\ndtype: int64\nNumber of sensors with unresolved CountyID:  SensorID      23\nSensorTown    23\nCountyID       0\ndtype: int64\n"
                }
            ], 
            "source": "print(\"Total number of sensors: \", SensorLocation.count())\nprint(\"Number of sensors with unresolved CountyID: \", SensorLocation[SensorLocation.isna().any(axis=1)].count())\n#print(\"List of unresolved sensors:\")\n#SensorLocation[SensorLocation.isna().any(axis=1)]\n#print(\"Number of duplicates in SensorID column: \", SensorLocation.duplicated(['SensorID']).sum())\n#SensorLocation.loc[SensorLocation.duplicated(['SensorID'])==True]"
        }, 
        {
            "source": "At the moment it is easier to drop these 3% of sensor's data. Otherwise this table could be corrected manually, since it has reasonable size and it's contents (sensor lables/county codes) hardly changes in time. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SensorID</th>\n      <th>SensorTown</th>\n      <th>CountyID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DEBB007</td>\n      <td>Elsterwerda</td>\n      <td>12062</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DEBB021</td>\n      <td>Potsdam</td>\n      <td>12054</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DEBB026</td>\n      <td>Spremberg</td>\n      <td>12071</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DEBB029</td>\n      <td>Schwedt/Oder</td>\n      <td>12073</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DEBB032</td>\n      <td>Eisenh\u00fcttenstadt</td>\n      <td>12067</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "  SensorID        SensorTown  CountyID\n0  DEBB007       Elsterwerda     12062\n1  DEBB021           Potsdam     12054\n2  DEBB026         Spremberg     12071\n3  DEBB029      Schwedt/Oder     12073\n4  DEBB032  Eisenh\u00fcttenstadt     12067"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "SensorLocation=SensorLocation.dropna()\nSensorLocation=SensorLocation.astype({'CountyID':int})\nSensorLocation.head(5)"
        }, 
        {
            "source": "### Saving Bronchial Asthma Prevalence DataFrames to COS\nNow we can save the resulting data set for further use:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "AsthmaSpark = spark.createDataFrame(dfAsthma)\nAsthmaSpark.write.parquet(cos.url('Asthma.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))"
        }, 
        {
            "source": "### Constructing \"Long\" DataFrame\nFor the further use in within the SparkSQL tools for the feature generation, it is worth to create a \"long\" Spark DataFrame,\ncontaining three columns (pollutant concentration, pollutant name and the county id) and many rows, one for each hourly-averaged measurement.\n\nFirst the initial **dffAll** DataFrame is converted to the \"long\" shape:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 16, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>observation_period</th>\n      <th>SensorPollID</th>\n      <th>PollutantConc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-01-01 00:00</td>\n      <td>DESH008_NO2_dataGroup1</td>\n      <td>35.767</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-01-01 01:00</td>\n      <td>DESH008_NO2_dataGroup1</td>\n      <td>34.950</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-01 02:00</td>\n      <td>DESH008_NO2_dataGroup1</td>\n      <td>34.773</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-01-01 03:00</td>\n      <td>DESH008_NO2_dataGroup1</td>\n      <td>36.938</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-01-01 04:00</td>\n      <td>DESH008_NO2_dataGroup1</td>\n      <td>36.090</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "  observation_period            SensorPollID  PollutantConc\n0   2016-01-01 00:00  DESH008_NO2_dataGroup1         35.767\n1   2016-01-01 01:00  DESH008_NO2_dataGroup1         34.950\n2   2016-01-01 02:00  DESH008_NO2_dataGroup1         34.773\n3   2016-01-01 03:00  DESH008_NO2_dataGroup1         36.938\n4   2016-01-01 04:00  DESH008_NO2_dataGroup1         36.090"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dffAllLong = pd.melt(dffAll, id_vars=['observation_period'], var_name='SensorPollID', value_name='PollutantConc')\ndffAllLong.head()"
        }, 
        {
            "source": "Then a dictionary for translation of *all* sensor's id's to the county id's is created:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import gc\ndel dffAll\ndel dff\ngc.collect()\n\nSensorCountyDict = dict(zip(SensorLocation.SensorID, SensorLocation.CountyID))"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#dffAllLong['CountyID'] = dffAllLong.apply(lambda row: re.search('(^.{7})', row['SensorPollID']).group(1), axis=1)\nColumnCountyID = pd.DataFrame()\nColumnSensorID = pd.DataFrame()\nColumnSensorID['SensorID'] = dffAllLong.apply(lambda row: re.search('(^.{7})', row['SensorPollID']).group(1), axis=1)\n#print(\"Memory usage: \", (ColumnSensorID.memory_usage(index=True).sum()/1048576.0), \" MB\")"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#ColumnCountyID.replace({'CountyID': SensorCountyDict})\nColumnCountyID['CountyID'] = ColumnSensorID['SensorID'].map(SensorCountyDict)\n#.dropna().astype('int64')\n#dffAllLong.replace({'CountyID': SensorCountyDict})\n#ColumnCountyID.head()"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Total number of sensors is  CountyID    507.0\ndtype: float64  ,  CountyID    8.0\ndtype: float64  of them have unresolved CountyID\n"
                }
            ], 
            "source": "print(\"Total number of sensors is \", (ColumnCountyID.count()/NumHoursInYear),\" , \", (ColumnCountyID.isna().sum()/NumHoursInYear), \" of them have unresolved CountyID\")"
        }, 
        {
            "source": "Now resolving the **SensorPollID** column of the melted DataFrame into the *double* **Pollutant** and *integer* **CountyID** columns:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 21, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PollutantConc</th>\n      <th>Pollutant</th>\n      <th>CountyID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4514971</th>\n      <td>0.50</td>\n      <td>SO2</td>\n      <td>12065</td>\n    </tr>\n    <tr>\n      <th>4514972</th>\n      <td>0.43</td>\n      <td>SO2</td>\n      <td>12065</td>\n    </tr>\n    <tr>\n      <th>4514973</th>\n      <td>0.47</td>\n      <td>SO2</td>\n      <td>12065</td>\n    </tr>\n    <tr>\n      <th>4514974</th>\n      <td>0.62</td>\n      <td>SO2</td>\n      <td>12065</td>\n    </tr>\n    <tr>\n      <th>4514975</th>\n      <td>0.80</td>\n      <td>SO2</td>\n      <td>12065</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "         PollutantConc Pollutant  CountyID\n4514971           0.50       SO2     12065\n4514972           0.43       SO2     12065\n4514973           0.47       SO2     12065\n4514974           0.62       SO2     12065\n4514975           0.80       SO2     12065"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dffAllLong['Pollutant'] = dffAllLong.apply(lambda row: re.search('^.{8}(.*)_', row['SensorPollID']).group(1), axis=1)\ndffAllLong['CountyID'] = ColumnCountyID['CountyID']\ndffAllLong = dffAllLong.dropna().drop(['observation_period','SensorPollID'], axis=1)\n#dffAllLong.iloc[888555]\ndffAllLong['CountyID'] = dffAllLong['CountyID'].astype('int64')\ndffAllLong.tail()\n#print(\"Memory usage: \", (dffAllLong.memory_usage(index=True).sum()/1048576.0), \" MB\")\n#SensorLocation.loc[SensorLocation['SensorID']=='DESL002']"
        }, 
        {
            "source": "### Saving \"Long\" DataFrame to the COS\nNow we can save the resulting dataset to the Cloud Object Storage for the further use:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dffAllLongSpark = spark.createDataFrame(dffAllLong)\ndffAllLongSpark.write.parquet(cos.url('dffAllLong.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}