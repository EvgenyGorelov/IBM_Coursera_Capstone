{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "\n# Advanced Data Science Capstone\n\n## Air pollution and prevalence of bronchial asthma in Germany  \n\n## Feature Creation and Feature engineering, Apache Spark SQL", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### The deliverables\nThe deliverables of the current stage:\n\n - Current notebook as the process documentation\n - Spark DataFrames with disease prevalence column, county id and features extracted from air pollution data series for sensors located in corresponding counties\n\n###  Feature creation\nThe basic features for air pollution levels are\n\n - Number of hours when pollutant concentration exceeded some certain value\n - Mean concentration of the pollutant\n - Median or other quantile concentration of the pollutant \n \n### Loading Apache Spark DataFrames from COS:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ndfAllLongSpark = spark.read.parquet(cos.url('dffAllLong.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfAsthmaSpark = spark.read.parquet(cos.url('Asthma.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))"
        }, 
        {
            "source": "###  Feature creation\n\nThe following features will be generated:\n - Pollutant concentration features\n   - Average concentration of every kind of pollutant over the year (average over all sensors within the county)\n   - 75th percentile of every kind pollutant over the year, that is also proportional to the number of hours when pollutant concentration exceeded some certain value\n - Health indicator features\n   - presense of the county in 50th, 75th or 95th percentile of bronchial asthma prevalence over the counties\n \nStarting from the \"long\" Apache Spark DataFrame **dfAllLongSpark** the quantities can be straightforward calculated by means of\nApache Spark SQL:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#sql view generation:\ndfAllLongSpark.createOrReplaceTempView(\"SensorsHour\")\ndfAsthmaSpark.createOrReplaceTempView(\"DiseaseCounty\")\n\n#spark.sql(\"select * from SensorsHour limit 10\").show()\n#spark.sql(\"select * from DiseaseCounty limit 10\").show() \n#dfAllLongSpark.printSchema()\n#dfAsthmaSpark.printSchema()"
        }, 
        {
            "source": "Commented chunk with quantile calculation examples:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#dfPollutantPercentilesSpark = spark.sql(\"\"\"\n#SELECT \n# distinct \n#     Pollutant, CountyID,\n#     AVG(PollutantConc) over(PARTITION BY Pollutant, CountyID) AS Mean,\n#     percentile_approx(PollutantConc,  0.5) over(PARTITION BY Pollutant, CountyID) as Percentile50\n#--     ,\n#--     percentile_approx(PollutantConc, 0.75) over(PARTITION BY Pollutant, CountyID) as Percentile75\n#FROM SensorsHour\n#\"\"\") \n\n#dfPollutantPercentilesSpark.createOrReplaceTempView(\"PollutantPercentiles\")\n#spark.sql(\"select * from PollutantPercentiles limit 10\").show()\n\n#spark.sql(\"\"\"\n#       SELECT \n#         CountyID,\n#         (case \n#            WHEN DiseaseR >=\n#             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n#          THEN 1\n#          ELSE 0\n#         END \n#        ) as DiseaseRFeat\n#     FROM DiseaseCounty\n#\"\"\").show(290)"
        }, 
        {
            "source": "Commented chunk with mean calculation example:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#spark.sql(\"\"\"\n#SELECT distinct \n#         CountyID,\n#         AVG(PollutantConc) over(PARTITION BY CountyID) AS NO\n#         FROM SensorsHour\n#         WHERE Pollutant='NO'\n#\"\"\").show(5)"
        }, 
        {
            "source": "### Calculation of feature matrices by means of Apache Spark SQL\nSelected feature sets (for the full set see Capstone.feature_eng.Pandas.X.X files) are generated by means of Apache Spark SQL.\nThe feature set DataFrame names are\n\ndfPol**XXXX**Disease**YY**perc, where\n\n**XXXX** = **MeanLong** denotes mean value of the limited pollutant set (*NO, NO2, PM1*)\n\n**XXXX** = **LongPerc75** denotes value of the 75th percentile for the limited pollutant set (*NO, NO2, PM1*)\n\n\n**YY** = 50 or 75 denotes that the high risk county flag is set, when the county is in 50th or 75th percentile of bronchial asthma prevalence over the counties\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease50perc : Pollutant mean; Disease level at or above 50th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolMeanLongDisease50perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.5) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease75perc : Pollutant mean; Disease level at or above 75th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolMeanLongDisease75perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolMeanLongDisease95perc : Pollutant mean; Disease level at or above 95th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolMeanLongDisease95perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.95) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          AVG(PollutantConc) over(PARTITION BY CountyID, Pollutant) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolLongPerc75Disease50perc : Pollutant level at or above 75th percentile; Disease level at or above 50th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolLongPerc75Disease50perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.5) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolLongPerc75Disease75perc : Pollutant level at or above 75th percentile; Disease level at or above 75th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolLongPerc75Disease75perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.75) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# PolLongPerc75Disease95perc : Pollutant level at or above 75th percentile; Disease level at or above 95th percentile\n# pollutant list: ListOfPollutantsLong = ['NO','NO2','PM1']\n\ndfPolLongPerc75Disease95perc = spark.sql(\"\"\"    \nSELECT t1.CountyID, t1.DiseaseRFeat, t2.NO, t3.NO2, t4.PM1 \nFROM\n       (SELECT \n         CountyID,\n         (case \n            WHEN DiseaseR >=\n             (select percentile(DiseaseR, 0.95) from DiseaseCounty limit 1) \n          THEN 1\n          ELSE 0\n         END \n        ) as DiseaseRFeat\n     FROM DiseaseCounty) t1\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO\n          FROM SensorsHour\n          WHERE Pollutant='NO') t2\n     ON t1.CountyID = t2.CountyID\n          JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS NO2\n          FROM SensorsHour\n          WHERE Pollutant='NO2') t3\n     ON t2.CountyID = t3.CountyID\n     JOIN\n        (SELECT distinct \n          CountyID, Pollutant,\n          percentile_approx(PollutantConc,  0.75) over(PARTITION BY Pollutant, CountyID) AS PM1\n          FROM SensorsHour\n          WHERE Pollutant='PM1') t4\n     ON t3.CountyID = t4.CountyID\n\n\"\"\")"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------+------------+---+---+---+\n|CountyID|DiseaseRFeat| NO|NO2|PM1|\n+--------+------------+---+---+---+\n|       0|           0|  0|  0|  0|\n+--------+------------+---+---+---+\n\n+--------+------------+------------+-----------+------------------+\n|CountyID|DiseaseRFeat|          NO|        NO2|               PM1|\n+--------+------------+------------+-----------+------------------+\n|   14626|           0|       7.785|     20.616|23.673000000000002|\n|   14521|           0|     288.893|     92.927|              64.8|\n|   16051|           1| 27.11467361|38.37479019|       22.60592461|\n|   14730|           0|      19.833|     54.754|            130.99|\n|    1002|           0|      74.347|       58.3|           126.083|\n|    1051|           1|     188.201|     80.934|            95.755|\n|   15002|           0|    34.32875|   44.47335|          28.09545|\n|   16076|           0|250.02636719|74.93490601|      209.64552307|\n|   15001|           0|     281.628|    88.5767|          759.8985|\n|   15084|           0|    228.8535|    96.2292|          540.1065|\n+--------+------------+------------+-----------+------------------+\n\n"
                }
            ], 
            "source": "from pyspark.sql.functions import isnan, when, count, col\n\ndftmp = dfPolLongPerc75Disease75perc\n\ndftmp.select([count(when(isnan(c), c)).alias(c) for c in dftmp.columns]).show()\n\ndftmp.createOrReplaceTempView(\"PolLongPerc75Disease75perc\")\nspark.sql(\"select * from PolLongPerc75Disease75perc limit 10\").show()"
        }, 
        {
            "source": "### Writing Spark DataSrames as Parquet to COS", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dfPolMeanLongDisease50perc.write.parquet(cos.url('dfPolMeanLongDisease50perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolMeanLongDisease75perc.write.parquet(cos.url('dfPolMeanLongDisease75perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolMeanLongDisease95perc.write.parquet(cos.url('dfPolMeanLongDisease95perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\n\ndfPolLongPerc75Disease50perc.write.parquet(cos.url('dfPolLongPerc75Disease50perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolLongPerc75Disease75perc.write.parquet(cos.url('dfPolLongPerc75Disease75perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))\ndfPolLongPerc75Disease95perc.write.parquet(cos.url('dfPolLongPerc75Disease95perc.parquet', 'capstone-donotdelete-pr-zpykcz8f0kxuad'))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}